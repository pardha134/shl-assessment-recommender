# Essential Files - Clean Project Structure

## âœ… Core Files Retained

### API (Main Interface)
```
api/
â”œâ”€â”€ main.py          # FastAPI application - START HERE
â””â”€â”€ schemas.py       # Request/Response schemas
```

### RAG System (Recommendation Engine)
```
rag/
â”œâ”€â”€ recommender.py   # Main recommendation logic
â”œâ”€â”€ retriever.py     # Vector similarity search
â””â”€â”€ prompt.py        # LLM prompt templates
```

### Vector Store (Search Index)
```
vector_store/
â”œâ”€â”€ vector_store.py      # FAISS vector store management
â”œâ”€â”€ query_processor.py   # Query embedding processor
â””â”€â”€ shl_faiss/          # Pre-built FAISS index (377 vectors)
    â”œâ”€â”€ index.faiss
    â”œâ”€â”€ embeddings.npy
    â””â”€â”€ metadata.json
```

### Embeddings (Vector Generation)
```
embeddings/
â”œâ”€â”€ build_embeddings.py  # Create embeddings from products
â””â”€â”€ load_embeddings.py   # Load embeddings utilities
```

### Data Processing
```
preprocessing/
â”œâ”€â”€ chunk_products.py    # Split products into chunks
â””â”€â”€ clean_text.py        # Text cleaning utilities

scraper/
â”œâ”€â”€ scrape_shl.py       # Web scraping
â””â”€â”€ parse_products.py   # Parse scraped data
```

### Data Files
```
data/
â”œâ”€â”€ processed/
â”‚   â””â”€â”€ shl_products.json    # 12 SHL products (processed)
â””â”€â”€ test_queries.csv         # Test dataset (10 queries)
```

### Predictions
```
predictions/
â””â”€â”€ Pardha_Saradhi_Thumma.csv  # Submission file âœ…
```

### Configuration & Scripts
```
config.py                    # Configuration management
run_pipeline.py             # Run complete data pipeline
export_predictions.py       # Export predictions utility
generate_submission.py      # Generate submission CSV
requirements.txt            # Python dependencies
.env                       # Environment variables (API keys)
.env.example              # Environment template
```

### Documentation
```
README.md                  # Main documentation
QUICKSTART.md             # Quick start guide
POSTMAN_GUIDE.md          # API testing with Postman
docs/
â””â”€â”€ API_DOCUMENTATION.md  # Complete API reference
```

---

## ğŸš€ Quick Start Commands

### 1. Start API Server
```bash
python api/main.py
```
Server runs at: `http://localhost:8000`

### 2. Test API Endpoint
```bash
curl -X POST "http://localhost:8000/recommend" \
  -H "Content-Type: application/json" \
  -d '{"query": "Hire software engineers", "top_k": 5}'
```

### 3. Generate Submission File
```bash
python generate_submission.py
```
Output: `predictions/Pardha_Saradhi_Thumma.csv`

### 4. Rebuild Pipeline (if needed)
```bash
python run_pipeline.py
```

---

## ğŸ“Š File Count Summary

| Category | Count |
|----------|-------|
| API Files | 2 |
| RAG System | 3 |
| Vector Store | 2 + index |
| Embeddings | 2 |
| Data Processing | 4 |
| Data Files | 2 |
| Scripts | 4 |
| Documentation | 4 |
| Configuration | 3 |
| **Total Essential** | **26 files** |

---

## ğŸ—‘ï¸ Removed Files

**Total Removed:** 23+ files including:
- Test scripts (3)
- Documentation files (8)
- Deployment files (3)
- Evaluation scripts (4)
- Web app (1)
- Redundant scripts (2)
- Raw data (1)
- Empty directories (3)

---

## âœ¨ Benefits

1. **Streamlined** - Only essential files remain
2. **Clear Purpose** - Each file has a specific role
3. **Easy Navigation** - Find what you need quickly
4. **Maintained Functionality** - All core features work
5. **Production Ready** - API endpoint fully functional

---

## ğŸ“ Key Endpoints

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/` | GET | API info |
| `/health` | GET | Health check |
| `/recommend` | POST | Get recommendations |
| `/docs` | GET | Interactive API docs |

---

## ğŸ¯ Main Workflow

```
1. Query â†’ API (/recommend)
2. API â†’ Query Processor (embed query)
3. Query Processor â†’ Vector Store (similarity search)
4. Vector Store â†’ Retriever (top-k results)
5. Retriever â†’ Recommender (LLM ranking)
6. Recommender â†’ API (JSON response)
7. API â†’ User (recommendations)
```

---

## ğŸ’¡ Notes

- Vector store is pre-built (no need to rebuild)
- API uses fallback mode if OpenAI quota exceeded
- Submission file already generated
- All dependencies in `requirements.txt`
- Environment variables in `.env`

---

## ğŸ”§ Maintenance

To update the system:
1. Modify product data in `data/processed/shl_products.json`
2. Run `python run_pipeline.py` to rebuild
3. Restart API: `python api/main.py`

---

**Project is now clean, organized, and production-ready! ğŸ‰**
